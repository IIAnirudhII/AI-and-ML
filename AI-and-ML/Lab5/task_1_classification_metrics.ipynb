{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Copy of task_1_classification_metrics.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IIAnirudhII/BU/blob/master/AI-and-ML/Lab5/task_1_classification_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFeluZ6z7OlE",
        "colab_type": "text"
      },
      "source": [
        "## Classification metrics\n",
        "\n",
        "Choosing right evaluation metrics for the problem is one of the most important aspect of machine learning. Choice of metrics allows us to compare performance of different models and helps in model selection.\n",
        "\n",
        "In this task, we will explore following metrics:\n",
        "- confusion matrix\n",
        "- accuracy\n",
        "- precision\n",
        "- recall\n",
        "- f1 score\n",
        "\n",
        "#### Dataset\n",
        "The training dataset is available at \"data/ozone_levels_train.csv\" in the respective challenge' repo.<br>\n",
        "The testing dataset is available at \"data/ozone_levels_test.csv\" in the respective challenge' repo.<br>\n",
        "\n",
        "The dataset is __modified version__ of the dataset 'ozone level' on provided by UCI Machine Learning repository.\n",
        "\n",
        "Original dataset: https://archive.ics.uci.edu/ml/datasets/Ozone+Level+Detection\n",
        "\n",
        "#### Objective\n",
        "To learn about classification metrics and compare logistic regression and decision tree on the same dataset\n",
        "\n",
        "#### Tasks\n",
        "- define X(input) and Y(output)\n",
        "- train the decision tree model \n",
        "- train the logistic model\n",
        "- construct a confusion matrix\n",
        "- calculate the classification accurace\n",
        "- calculate the Precision\n",
        "- calculate the Recall\n",
        "- calculate the F1 score\n",
        "- calculate Area Under ROC Curve\n",
        "\n",
        "#### Further fun\n",
        "- Calculate precission and recall\n",
        "- find the area under the curve for Roc metrics\n",
        "- impliment below metrics using inbuilt librarires\n",
        "        confusion matrix\n",
        "        accuracy\n",
        "        precision\n",
        "        recall\n",
        "        f1 score\n",
        "\n",
        "\n",
        "#### Helpful links\n",
        "- Classification metrics with google developers: https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative\n",
        "- classification metrics: https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html\n",
        "- pd.get_dummies() and One Hot Encoding: https://queirozf.com/entries/one-hot-encoding-a-feature-on-a-pandas-dataframe-an-example\n",
        "- Differences between Logistic Regression and a Decision Tree: https://www.geeksforgeeks.org/ml-logistic-regression-v-s-decision-tree-classification/\n",
        "- Decision Tree Classifier by Sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "- Understanding classification metrics like Precision, Recall, F-Scores and Confusion matrices: https://nillsf.com/index.php/2020/05/23/confusion-matrix-accuracy-recall-precision-false-positive-rate-and-f-scores-explained/\n",
        "- Understanding the ROC Curve: https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
        "- Use slack for doubts: https://join.slack.com/t/deepconnectai/shared_invite/zt-givlfnf6-~cn3SQ43k0BGDrG9_YOn4g"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjJPRaRc7OlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# Uncomment below 2 lines to ignore warnings\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuyu4khV7OlO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "c0f4c249-c719-42fe-e66b-f5738ad75ef2"
      },
      "source": [
        "# Download data using wget if running on cloud\n",
        "!wget https://github.com/DeepConnectAI/challenge-week-5/raw/master/data/ozone_levels_train.csv\n",
        "!wget https://github.com/DeepConnectAI/challenge-week-5/raw/master/data/ozone_levels_test.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-12 13:08:37--  https://github.com/DeepConnectAI/challenge-week-5/raw/master/data/ozone_levels_train.csv\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/DeepConnectAI/challenge-week-5/master/data/ozone_levels_train.csv [following]\n",
            "--2020-09-12 13:08:37--  https://raw.githubusercontent.com/DeepConnectAI/challenge-week-5/master/data/ozone_levels_train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 762464 (745K) [text/plain]\n",
            "Saving to: ‘ozone_levels_train.csv’\n",
            "\n",
            "ozone_levels_train. 100%[===================>] 744.59K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2020-09-12 13:08:38 (11.5 MB/s) - ‘ozone_levels_train.csv’ saved [762464/762464]\n",
            "\n",
            "--2020-09-12 13:08:38--  https://github.com/DeepConnectAI/challenge-week-5/raw/master/data/ozone_levels_test.csv\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/DeepConnectAI/challenge-week-5/master/data/ozone_levels_test.csv [following]\n",
            "--2020-09-12 13:08:38--  https://raw.githubusercontent.com/DeepConnectAI/challenge-week-5/master/data/ozone_levels_test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 326448 (319K) [text/plain]\n",
            "Saving to: ‘ozone_levels_test.csv’\n",
            "\n",
            "ozone_levels_test.c 100%[===================>] 318.80K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-09-12 13:08:38 (8.16 MB/s) - ‘ozone_levels_test.csv’ saved [326448/326448]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDJy_ync7OlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the train and test data\n",
        "train = pd.read_csv(\"ozone_levels_train.csv\");\n",
        "test  = pd.read_csv(\"ozone_levels_test.csv\");"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wnrMC_K7Ole",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7075951e-868f-4027-898d-9a1f834bde7c"
      },
      "source": [
        "# Explore train dataset\n",
        "train.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1775, 73)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY2sP7la7Olj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "4eabad3a-d2c8-45c4-9021-0d65407be183"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>F_0</th>\n",
              "      <th>F_1</th>\n",
              "      <th>F_2</th>\n",
              "      <th>F_3</th>\n",
              "      <th>F_4</th>\n",
              "      <th>F_5</th>\n",
              "      <th>F_6</th>\n",
              "      <th>F_7</th>\n",
              "      <th>F_8</th>\n",
              "      <th>F_9</th>\n",
              "      <th>F_10</th>\n",
              "      <th>F_11</th>\n",
              "      <th>F_12</th>\n",
              "      <th>F_13</th>\n",
              "      <th>F_14</th>\n",
              "      <th>F_15</th>\n",
              "      <th>F_16</th>\n",
              "      <th>F_17</th>\n",
              "      <th>F_18</th>\n",
              "      <th>F_19</th>\n",
              "      <th>F_20</th>\n",
              "      <th>F_21</th>\n",
              "      <th>F_22</th>\n",
              "      <th>F_23</th>\n",
              "      <th>F_24</th>\n",
              "      <th>F_25</th>\n",
              "      <th>F_26</th>\n",
              "      <th>F_27</th>\n",
              "      <th>F_28</th>\n",
              "      <th>F_29</th>\n",
              "      <th>F_30</th>\n",
              "      <th>F_31</th>\n",
              "      <th>F_32</th>\n",
              "      <th>F_33</th>\n",
              "      <th>F_34</th>\n",
              "      <th>F_35</th>\n",
              "      <th>F_36</th>\n",
              "      <th>F_37</th>\n",
              "      <th>F_38</th>\n",
              "      <th>F_39</th>\n",
              "      <th>F_40</th>\n",
              "      <th>F_41</th>\n",
              "      <th>F_42</th>\n",
              "      <th>F_43</th>\n",
              "      <th>F_44</th>\n",
              "      <th>F_45</th>\n",
              "      <th>F_46</th>\n",
              "      <th>F_47</th>\n",
              "      <th>F_48</th>\n",
              "      <th>F_49</th>\n",
              "      <th>F_50</th>\n",
              "      <th>F_51</th>\n",
              "      <th>F_52</th>\n",
              "      <th>F_53</th>\n",
              "      <th>F_54</th>\n",
              "      <th>F_55</th>\n",
              "      <th>F_56</th>\n",
              "      <th>F_57</th>\n",
              "      <th>F_58</th>\n",
              "      <th>F_59</th>\n",
              "      <th>F_60</th>\n",
              "      <th>F_61</th>\n",
              "      <th>F_62</th>\n",
              "      <th>F_63</th>\n",
              "      <th>F_64</th>\n",
              "      <th>F_65</th>\n",
              "      <th>F_66</th>\n",
              "      <th>F_67</th>\n",
              "      <th>F_68</th>\n",
              "      <th>F_69</th>\n",
              "      <th>F_70</th>\n",
              "      <th>F_71</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4.4</td>\n",
              "      <td>4.6</td>\n",
              "      <td>4.4</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4.2</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.2</td>\n",
              "      <td>4.6</td>\n",
              "      <td>3.9</td>\n",
              "      <td>3.8</td>\n",
              "      <td>3.6</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.6</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>3.2</td>\n",
              "      <td>2.8</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.1</td>\n",
              "      <td>2.1</td>\n",
              "      <td>2.1</td>\n",
              "      <td>2.6</td>\n",
              "      <td>4.6</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>-0.7</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>-1.1</td>\n",
              "      <td>-1.2</td>\n",
              "      <td>-1.1</td>\n",
              "      <td>-0.7</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2.7</td>\n",
              "      <td>3.6</td>\n",
              "      <td>2.7</td>\n",
              "      <td>1.9</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>3.6</td>\n",
              "      <td>0.3</td>\n",
              "      <td>3.9</td>\n",
              "      <td>0.22</td>\n",
              "      <td>4.94</td>\n",
              "      <td>-3.14</td>\n",
              "      <td>1557.5</td>\n",
              "      <td>-3.1</td>\n",
              "      <td>0.09</td>\n",
              "      <td>11.71</td>\n",
              "      <td>-1.07</td>\n",
              "      <td>3113.0</td>\n",
              "      <td>-16.8</td>\n",
              "      <td>0.07</td>\n",
              "      <td>26.29</td>\n",
              "      <td>-2.37</td>\n",
              "      <td>5705.0</td>\n",
              "      <td>-19.40</td>\n",
              "      <td>23.40</td>\n",
              "      <td>10315.0</td>\n",
              "      <td>-0.130416</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.2</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.1</td>\n",
              "      <td>1.9</td>\n",
              "      <td>2.4</td>\n",
              "      <td>5.1</td>\n",
              "      <td>5.3</td>\n",
              "      <td>5.1</td>\n",
              "      <td>4.4</td>\n",
              "      <td>4.6</td>\n",
              "      <td>4.6</td>\n",
              "      <td>4.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1.6</td>\n",
              "      <td>5.3</td>\n",
              "      <td>2.6</td>\n",
              "      <td>7.3</td>\n",
              "      <td>7.2</td>\n",
              "      <td>5.8</td>\n",
              "      <td>4.9</td>\n",
              "      <td>4.6</td>\n",
              "      <td>4.2</td>\n",
              "      <td>4.4</td>\n",
              "      <td>6.1</td>\n",
              "      <td>8.6</td>\n",
              "      <td>12.8</td>\n",
              "      <td>15.9</td>\n",
              "      <td>16.8</td>\n",
              "      <td>17.3</td>\n",
              "      <td>16.6</td>\n",
              "      <td>16.3</td>\n",
              "      <td>16.2</td>\n",
              "      <td>15.9</td>\n",
              "      <td>15.4</td>\n",
              "      <td>15.1</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>14.8</td>\n",
              "      <td>15.2</td>\n",
              "      <td>15.2</td>\n",
              "      <td>17.3</td>\n",
              "      <td>11.9</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.15</td>\n",
              "      <td>2.02</td>\n",
              "      <td>-0.59</td>\n",
              "      <td>1510.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.17</td>\n",
              "      <td>10.11</td>\n",
              "      <td>-2.63</td>\n",
              "      <td>3083.0</td>\n",
              "      <td>-16.0</td>\n",
              "      <td>0.24</td>\n",
              "      <td>15.80</td>\n",
              "      <td>2.10</td>\n",
              "      <td>5710.0</td>\n",
              "      <td>-17.50</td>\n",
              "      <td>19.00</td>\n",
              "      <td>10210.0</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1.3</td>\n",
              "      <td>2.3</td>\n",
              "      <td>2.4</td>\n",
              "      <td>2.9</td>\n",
              "      <td>2.9</td>\n",
              "      <td>2.8</td>\n",
              "      <td>3.2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.6</td>\n",
              "      <td>12.3</td>\n",
              "      <td>11.7</td>\n",
              "      <td>11.3</td>\n",
              "      <td>11.3</td>\n",
              "      <td>11.2</td>\n",
              "      <td>12.2</td>\n",
              "      <td>11.7</td>\n",
              "      <td>14.6</td>\n",
              "      <td>17.7</td>\n",
              "      <td>20.0</td>\n",
              "      <td>21.3</td>\n",
              "      <td>22.3</td>\n",
              "      <td>23.2</td>\n",
              "      <td>23.7</td>\n",
              "      <td>24.7</td>\n",
              "      <td>25.2</td>\n",
              "      <td>25.3</td>\n",
              "      <td>25.0</td>\n",
              "      <td>23.6</td>\n",
              "      <td>20.9</td>\n",
              "      <td>17.4</td>\n",
              "      <td>15.9</td>\n",
              "      <td>14.5</td>\n",
              "      <td>14.4</td>\n",
              "      <td>25.3</td>\n",
              "      <td>18.0</td>\n",
              "      <td>9.9</td>\n",
              "      <td>0.20</td>\n",
              "      <td>6.48</td>\n",
              "      <td>-12.56</td>\n",
              "      <td>1520.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.07</td>\n",
              "      <td>15.06</td>\n",
              "      <td>-15.37</td>\n",
              "      <td>3105.5</td>\n",
              "      <td>-13.3</td>\n",
              "      <td>0.12</td>\n",
              "      <td>31.64</td>\n",
              "      <td>-5.24</td>\n",
              "      <td>5745.0</td>\n",
              "      <td>-16.90</td>\n",
              "      <td>25.90</td>\n",
              "      <td>10175.0</td>\n",
              "      <td>85.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.6</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.6</td>\n",
              "      <td>2.6</td>\n",
              "      <td>2.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>2.8</td>\n",
              "      <td>1.7</td>\n",
              "      <td>1.1</td>\n",
              "      <td>1.1</td>\n",
              "      <td>1.3</td>\n",
              "      <td>1.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.7</td>\n",
              "      <td>18.3</td>\n",
              "      <td>17.7</td>\n",
              "      <td>17.3</td>\n",
              "      <td>16.6</td>\n",
              "      <td>16.3</td>\n",
              "      <td>16.2</td>\n",
              "      <td>15.7</td>\n",
              "      <td>16.6</td>\n",
              "      <td>18.7</td>\n",
              "      <td>20.8</td>\n",
              "      <td>23.2</td>\n",
              "      <td>25.0</td>\n",
              "      <td>25.8</td>\n",
              "      <td>26.3</td>\n",
              "      <td>26.4</td>\n",
              "      <td>26.4</td>\n",
              "      <td>25.9</td>\n",
              "      <td>24.5</td>\n",
              "      <td>22.9</td>\n",
              "      <td>21.7</td>\n",
              "      <td>20.8</td>\n",
              "      <td>20.1</td>\n",
              "      <td>19.2</td>\n",
              "      <td>17.6</td>\n",
              "      <td>26.4</td>\n",
              "      <td>20.8</td>\n",
              "      <td>11.1</td>\n",
              "      <td>0.69</td>\n",
              "      <td>-5.85</td>\n",
              "      <td>-5.37</td>\n",
              "      <td>1579.5</td>\n",
              "      <td>5.2</td>\n",
              "      <td>0.03</td>\n",
              "      <td>-2.70</td>\n",
              "      <td>-0.73</td>\n",
              "      <td>3179.0</td>\n",
              "      <td>-12.7</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.22</td>\n",
              "      <td>-0.28</td>\n",
              "      <td>5835.0</td>\n",
              "      <td>-9.55</td>\n",
              "      <td>42.15</td>\n",
              "      <td>10215.0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.9</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1.1</td>\n",
              "      <td>2.3</td>\n",
              "      <td>2.8</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.8</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.2</td>\n",
              "      <td>21.6</td>\n",
              "      <td>21.0</td>\n",
              "      <td>20.7</td>\n",
              "      <td>20.4</td>\n",
              "      <td>20.2</td>\n",
              "      <td>20.2</td>\n",
              "      <td>21.7</td>\n",
              "      <td>23.3</td>\n",
              "      <td>24.9</td>\n",
              "      <td>26.8</td>\n",
              "      <td>28.2</td>\n",
              "      <td>28.5</td>\n",
              "      <td>23.3</td>\n",
              "      <td>21.2</td>\n",
              "      <td>21.7</td>\n",
              "      <td>22.0</td>\n",
              "      <td>22.8</td>\n",
              "      <td>23.1</td>\n",
              "      <td>23.0</td>\n",
              "      <td>22.4</td>\n",
              "      <td>21.9</td>\n",
              "      <td>22.2</td>\n",
              "      <td>21.9</td>\n",
              "      <td>21.4</td>\n",
              "      <td>28.5</td>\n",
              "      <td>22.7</td>\n",
              "      <td>15.5</td>\n",
              "      <td>0.70</td>\n",
              "      <td>3.86</td>\n",
              "      <td>2.62</td>\n",
              "      <td>1557.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>0.79</td>\n",
              "      <td>7.00</td>\n",
              "      <td>0.70</td>\n",
              "      <td>3171.5</td>\n",
              "      <td>-10.9</td>\n",
              "      <td>0.87</td>\n",
              "      <td>1.65</td>\n",
              "      <td>1.51</td>\n",
              "      <td>5835.0</td>\n",
              "      <td>32.95</td>\n",
              "      <td>47.30</td>\n",
              "      <td>10170.0</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   F_0  F_1  F_2  F_3  F_4  F_5  ...   F_67   F_68     F_69       F_70  F_71  class\n",
              "0  2.5  3.5  4.4  4.6  4.4  3.5  ... -19.40  23.40  10315.0  -0.130416  0.00    0.0\n",
              "1  1.2  0.7  0.3  0.1  0.3  0.4  ... -17.50  19.00  10210.0  15.000000  0.00    0.0\n",
              "2  0.1  0.4  0.6  0.4  1.0  1.7  ... -16.90  25.90  10175.0  85.000000  0.00    0.0\n",
              "3  0.6  0.9  1.0  0.6  0.7  0.7  ...  -9.55  42.15  10215.0   5.000000  0.00    0.0\n",
              "4  0.1  0.4  0.3  0.1  0.1  0.0  ...  32.95  47.30  10170.0  15.000000  0.97    0.0\n",
              "\n",
              "[5 rows x 73 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOJNTN_E7Olo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1b1dd666-8d84-43d7-fb42-9d0076f7d82a"
      },
      "source": [
        "# Explore test dataset\n",
        "test.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(761, 73)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3-WA8tN7Olv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "59527509-7e7e-4985-c22b-320276fd6ed0"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>F_0</th>\n",
              "      <th>F_1</th>\n",
              "      <th>F_2</th>\n",
              "      <th>F_3</th>\n",
              "      <th>F_4</th>\n",
              "      <th>F_5</th>\n",
              "      <th>F_6</th>\n",
              "      <th>F_7</th>\n",
              "      <th>F_8</th>\n",
              "      <th>F_9</th>\n",
              "      <th>F_10</th>\n",
              "      <th>F_11</th>\n",
              "      <th>F_12</th>\n",
              "      <th>F_13</th>\n",
              "      <th>F_14</th>\n",
              "      <th>F_15</th>\n",
              "      <th>F_16</th>\n",
              "      <th>F_17</th>\n",
              "      <th>F_18</th>\n",
              "      <th>F_19</th>\n",
              "      <th>F_20</th>\n",
              "      <th>F_21</th>\n",
              "      <th>F_22</th>\n",
              "      <th>F_23</th>\n",
              "      <th>F_24</th>\n",
              "      <th>F_25</th>\n",
              "      <th>F_26</th>\n",
              "      <th>F_27</th>\n",
              "      <th>F_28</th>\n",
              "      <th>F_29</th>\n",
              "      <th>F_30</th>\n",
              "      <th>F_31</th>\n",
              "      <th>F_32</th>\n",
              "      <th>F_33</th>\n",
              "      <th>F_34</th>\n",
              "      <th>F_35</th>\n",
              "      <th>F_36</th>\n",
              "      <th>F_37</th>\n",
              "      <th>F_38</th>\n",
              "      <th>F_39</th>\n",
              "      <th>F_40</th>\n",
              "      <th>F_41</th>\n",
              "      <th>F_42</th>\n",
              "      <th>F_43</th>\n",
              "      <th>F_44</th>\n",
              "      <th>F_45</th>\n",
              "      <th>F_46</th>\n",
              "      <th>F_47</th>\n",
              "      <th>F_48</th>\n",
              "      <th>F_49</th>\n",
              "      <th>F_50</th>\n",
              "      <th>F_51</th>\n",
              "      <th>F_52</th>\n",
              "      <th>F_53</th>\n",
              "      <th>F_54</th>\n",
              "      <th>F_55</th>\n",
              "      <th>F_56</th>\n",
              "      <th>F_57</th>\n",
              "      <th>F_58</th>\n",
              "      <th>F_59</th>\n",
              "      <th>F_60</th>\n",
              "      <th>F_61</th>\n",
              "      <th>F_62</th>\n",
              "      <th>F_63</th>\n",
              "      <th>F_64</th>\n",
              "      <th>F_65</th>\n",
              "      <th>F_66</th>\n",
              "      <th>F_67</th>\n",
              "      <th>F_68</th>\n",
              "      <th>F_69</th>\n",
              "      <th>F_70</th>\n",
              "      <th>F_71</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.0</td>\n",
              "      <td>3.7</td>\n",
              "      <td>2.9</td>\n",
              "      <td>3.6</td>\n",
              "      <td>2.4</td>\n",
              "      <td>2.9</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>3.8</td>\n",
              "      <td>3.9</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.7</td>\n",
              "      <td>4.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.7</td>\n",
              "      <td>4.4</td>\n",
              "      <td>4.4</td>\n",
              "      <td>2.8</td>\n",
              "      <td>3.1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>1.7</td>\n",
              "      <td>1.6</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.3</td>\n",
              "      <td>3.3</td>\n",
              "      <td>2.9</td>\n",
              "      <td>2.4</td>\n",
              "      <td>2.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2.2</td>\n",
              "      <td>3.6</td>\n",
              "      <td>4.6</td>\n",
              "      <td>5.6</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.3</td>\n",
              "      <td>6.5</td>\n",
              "      <td>6.3</td>\n",
              "      <td>5.3</td>\n",
              "      <td>4.5</td>\n",
              "      <td>3.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>2.1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.5</td>\n",
              "      <td>3.2</td>\n",
              "      <td>-2.60</td>\n",
              "      <td>0.81</td>\n",
              "      <td>-2.22</td>\n",
              "      <td>-5.69</td>\n",
              "      <td>1488.0</td>\n",
              "      <td>-4.10</td>\n",
              "      <td>0.89</td>\n",
              "      <td>15.50</td>\n",
              "      <td>1.69</td>\n",
              "      <td>3025.0</td>\n",
              "      <td>-19.8</td>\n",
              "      <td>0.47</td>\n",
              "      <td>27.66</td>\n",
              "      <td>11.94</td>\n",
              "      <td>5605.0</td>\n",
              "      <td>10.70</td>\n",
              "      <td>31.95</td>\n",
              "      <td>10240.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.2</td>\n",
              "      <td>2.9</td>\n",
              "      <td>3.4</td>\n",
              "      <td>4.2</td>\n",
              "      <td>4.7</td>\n",
              "      <td>4.7</td>\n",
              "      <td>5.3</td>\n",
              "      <td>4.9</td>\n",
              "      <td>5.2</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.9</td>\n",
              "      <td>6.1</td>\n",
              "      <td>6.8</td>\n",
              "      <td>6.3</td>\n",
              "      <td>6.3</td>\n",
              "      <td>6.4</td>\n",
              "      <td>5.7</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.5</td>\n",
              "      <td>6.8</td>\n",
              "      <td>3.9</td>\n",
              "      <td>5.5</td>\n",
              "      <td>5.8</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.9</td>\n",
              "      <td>6.1</td>\n",
              "      <td>6.3</td>\n",
              "      <td>6.6</td>\n",
              "      <td>6.9</td>\n",
              "      <td>7.8</td>\n",
              "      <td>9.1</td>\n",
              "      <td>10.5</td>\n",
              "      <td>11.6</td>\n",
              "      <td>12.8</td>\n",
              "      <td>13.9</td>\n",
              "      <td>14.7</td>\n",
              "      <td>14.9</td>\n",
              "      <td>14.2</td>\n",
              "      <td>12.2</td>\n",
              "      <td>10.1</td>\n",
              "      <td>8.8</td>\n",
              "      <td>7.1</td>\n",
              "      <td>5.9</td>\n",
              "      <td>6.1</td>\n",
              "      <td>14.9</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.35</td>\n",
              "      <td>0.73</td>\n",
              "      <td>16.14</td>\n",
              "      <td>4.53</td>\n",
              "      <td>1382.5</td>\n",
              "      <td>-2.45</td>\n",
              "      <td>0.08</td>\n",
              "      <td>18.29</td>\n",
              "      <td>8.03</td>\n",
              "      <td>2933.0</td>\n",
              "      <td>-20.1</td>\n",
              "      <td>0.20</td>\n",
              "      <td>19.22</td>\n",
              "      <td>18.21</td>\n",
              "      <td>5515.0</td>\n",
              "      <td>-10.10</td>\n",
              "      <td>42.00</td>\n",
              "      <td>10065.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.7</td>\n",
              "      <td>2.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.6</td>\n",
              "      <td>2.9</td>\n",
              "      <td>3.2</td>\n",
              "      <td>2.9</td>\n",
              "      <td>3.6</td>\n",
              "      <td>4.2</td>\n",
              "      <td>4.8</td>\n",
              "      <td>5.1</td>\n",
              "      <td>5.1</td>\n",
              "      <td>5.4</td>\n",
              "      <td>5.3</td>\n",
              "      <td>5.3</td>\n",
              "      <td>4.5</td>\n",
              "      <td>3.8</td>\n",
              "      <td>4.3</td>\n",
              "      <td>6.6</td>\n",
              "      <td>5.7</td>\n",
              "      <td>5.8</td>\n",
              "      <td>5.5</td>\n",
              "      <td>5.8</td>\n",
              "      <td>6.6</td>\n",
              "      <td>4.3</td>\n",
              "      <td>19.6</td>\n",
              "      <td>19.6</td>\n",
              "      <td>19.4</td>\n",
              "      <td>19.6</td>\n",
              "      <td>19.5</td>\n",
              "      <td>19.7</td>\n",
              "      <td>19.9</td>\n",
              "      <td>20.2</td>\n",
              "      <td>20.8</td>\n",
              "      <td>21.7</td>\n",
              "      <td>22.4</td>\n",
              "      <td>23.3</td>\n",
              "      <td>24.0</td>\n",
              "      <td>24.9</td>\n",
              "      <td>25.7</td>\n",
              "      <td>25.9</td>\n",
              "      <td>26.3</td>\n",
              "      <td>26.6</td>\n",
              "      <td>26.0</td>\n",
              "      <td>24.1</td>\n",
              "      <td>22.7</td>\n",
              "      <td>21.5</td>\n",
              "      <td>20.0</td>\n",
              "      <td>18.6</td>\n",
              "      <td>26.6</td>\n",
              "      <td>22.2</td>\n",
              "      <td>14.10</td>\n",
              "      <td>0.30</td>\n",
              "      <td>10.00</td>\n",
              "      <td>5.23</td>\n",
              "      <td>1471.0</td>\n",
              "      <td>2.60</td>\n",
              "      <td>0.30</td>\n",
              "      <td>11.28</td>\n",
              "      <td>0.54</td>\n",
              "      <td>3076.5</td>\n",
              "      <td>-16.5</td>\n",
              "      <td>0.10</td>\n",
              "      <td>14.22</td>\n",
              "      <td>-2.98</td>\n",
              "      <td>5690.0</td>\n",
              "      <td>0.70</td>\n",
              "      <td>32.70</td>\n",
              "      <td>10105.0</td>\n",
              "      <td>-55.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.5</td>\n",
              "      <td>1.3</td>\n",
              "      <td>1.8</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.7</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1.6</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>3.2</td>\n",
              "      <td>3.3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.1</td>\n",
              "      <td>3.4</td>\n",
              "      <td>3.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.1</td>\n",
              "      <td>3.2</td>\n",
              "      <td>2.8</td>\n",
              "      <td>3.1</td>\n",
              "      <td>3.2</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>18.8</td>\n",
              "      <td>18.8</td>\n",
              "      <td>19.7</td>\n",
              "      <td>20.1</td>\n",
              "      <td>20.0</td>\n",
              "      <td>19.8</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>20.3</td>\n",
              "      <td>19.8</td>\n",
              "      <td>21.2</td>\n",
              "      <td>22.3</td>\n",
              "      <td>23.0</td>\n",
              "      <td>23.5</td>\n",
              "      <td>23.1</td>\n",
              "      <td>22.4</td>\n",
              "      <td>21.7</td>\n",
              "      <td>20.3</td>\n",
              "      <td>19.5</td>\n",
              "      <td>20.2</td>\n",
              "      <td>20.9</td>\n",
              "      <td>21.2</td>\n",
              "      <td>20.8</td>\n",
              "      <td>20.4</td>\n",
              "      <td>23.5</td>\n",
              "      <td>20.7</td>\n",
              "      <td>10.10</td>\n",
              "      <td>0.74</td>\n",
              "      <td>2.03</td>\n",
              "      <td>8.12</td>\n",
              "      <td>1566.0</td>\n",
              "      <td>4.90</td>\n",
              "      <td>0.28</td>\n",
              "      <td>6.91</td>\n",
              "      <td>2.43</td>\n",
              "      <td>3155.0</td>\n",
              "      <td>-11.9</td>\n",
              "      <td>0.54</td>\n",
              "      <td>13.07</td>\n",
              "      <td>9.15</td>\n",
              "      <td>5820.0</td>\n",
              "      <td>1.95</td>\n",
              "      <td>39.35</td>\n",
              "      <td>10220.0</td>\n",
              "      <td>-25.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.6</td>\n",
              "      <td>2.7</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>2.6</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4.2</td>\n",
              "      <td>4.5</td>\n",
              "      <td>3.8</td>\n",
              "      <td>3.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.7</td>\n",
              "      <td>2.1</td>\n",
              "      <td>2.4</td>\n",
              "      <td>2.1</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1.6</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.9</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.9</td>\n",
              "      <td>2.3</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2.7</td>\n",
              "      <td>28.1</td>\n",
              "      <td>27.6</td>\n",
              "      <td>27.2</td>\n",
              "      <td>26.9</td>\n",
              "      <td>26.9</td>\n",
              "      <td>26.6</td>\n",
              "      <td>26.8</td>\n",
              "      <td>27.8</td>\n",
              "      <td>29.2</td>\n",
              "      <td>30.4</td>\n",
              "      <td>31.3</td>\n",
              "      <td>32.2</td>\n",
              "      <td>33.3</td>\n",
              "      <td>34.2</td>\n",
              "      <td>35.1</td>\n",
              "      <td>35.6</td>\n",
              "      <td>35.3</td>\n",
              "      <td>34.9</td>\n",
              "      <td>33.6</td>\n",
              "      <td>32.2</td>\n",
              "      <td>30.9</td>\n",
              "      <td>29.9</td>\n",
              "      <td>28.9</td>\n",
              "      <td>28.1</td>\n",
              "      <td>35.6</td>\n",
              "      <td>30.5</td>\n",
              "      <td>20.10</td>\n",
              "      <td>0.66</td>\n",
              "      <td>2.01</td>\n",
              "      <td>0.68</td>\n",
              "      <td>1529.5</td>\n",
              "      <td>11.30</td>\n",
              "      <td>0.47</td>\n",
              "      <td>-1.34</td>\n",
              "      <td>-1.14</td>\n",
              "      <td>3182.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.26</td>\n",
              "      <td>-1.52</td>\n",
              "      <td>-4.53</td>\n",
              "      <td>5910.0</td>\n",
              "      <td>27.70</td>\n",
              "      <td>43.70</td>\n",
              "      <td>10110.0</td>\n",
              "      <td>-30.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   F_0  F_1  F_2  F_3  F_4  F_5  ...   F_67   F_68     F_69  F_70  F_71  class\n",
              "0  4.0  3.7  2.9  3.6  2.4  2.9  ...  10.70  31.95  10240.0  10.0   0.0    0.0\n",
              "1  2.2  2.9  3.4  4.2  4.7  4.7  ... -10.10  42.00  10065.0  25.0   0.0    0.0\n",
              "2  2.7  2.2  2.3  2.5  2.6  2.9  ...   0.70  32.70  10105.0 -55.0   0.0    0.0\n",
              "3  1.5  1.3  1.8  1.4  1.2  1.7  ...   1.95  39.35  10220.0 -25.0   0.0    0.0\n",
              "4  2.6  2.7  2.2  1.4  1.6  1.9  ...  27.70  43.70  10110.0 -30.0   0.0    0.0\n",
              "\n",
              "[5 rows x 73 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPm-wKOh7Oly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define X and y\n",
        "X_train = train.iloc[:,:-1].values\n",
        "X_test  = test.iloc[:,:-1].values\n",
        "y_train = train.iloc[:,-1].values\n",
        "y_test  = test.iloc[:,-1].values"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyHrDvq07Ol3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "b88fbc82-b734-4edd-d7b8-4b29036bb93a"
      },
      "source": [
        "# Print shape of X_train, X_test, y_train, y_test\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1775, 72)\n",
            "(761, 72)\n",
            "(1775,)\n",
            "(761,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGOAKpHq7Ol7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the models\n",
        "# Classifier 1 - Logistic regression\n",
        "clf1 = LogisticRegression(max_iter=10000)\n",
        "# Classifier 2 - Decision tree\n",
        "clf2 = DecisionTreeClassifier()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neiuwNGF7OmA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "a2d759c2-4da2-4ebc-bfe8-489dbc83d283"
      },
      "source": [
        "# Train both the models on training dataset\n",
        "clf1.fit(X_train,y_train)\n",
        "\n",
        "clf2.fit(X_train,y_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8FPlfA87OmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict on testing data\n",
        "y_pred_lr = clf1.predict(X_test)\n",
        "y_pred_dt = clf2.predict(X_test)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm-nCJ5e7OmJ",
        "colab_type": "text"
      },
      "source": [
        "### Primary building blocks of classification metrics\n",
        "\n",
        "A __TRUE POSITIVE (TP)__ is an outcome where the model correctly predicts the positive class.\n",
        "\n",
        "A __TRUE NEGATIVE (TN)__ is an outcome where the model correctly predicts the negative class.\n",
        "\n",
        "A __FALSE POSITIVE (FP)__ is an outcome where the model incorrectly predicts the positive class.\n",
        "\n",
        "a __FALSE NEGATIVE (FN)__ is an outcome where the model incorrectly predicts the negative class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ONNi__CS0CP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b8b07329-56f0-48b3-cfe1-f3496525f555"
      },
      "source": [
        "lr_confusion_matrix=confusion_matrix(y_test,y_pred_lr)\n",
        "print(lr_confusion_matrix)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[729  11]\n",
            " [ 18   3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRg4Qepo7OmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute primary metrics for logisitc regression\n",
        "# NOTE: All metrics are to be calculated on test dataset\n",
        "# True Positive\n",
        "lr_true_positive = lr_confusion_matrix[1,1]\n",
        "# True Negative\n",
        "lr_true_negative = lr_confusion_matrix[0,0]\n",
        "# False Positive\n",
        "lr_false_positive = lr_confusion_matrix[0,1]\n",
        "# False Negative\n",
        "lr_false_negative = lr_confusion_matrix[1,0]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA2rmciydp5y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ae1fce1a-54c3-430f-cce4-52bec2bb687d"
      },
      "source": [
        "dt_confusion_matrix=confusion_matrix(y_test,y_pred_dt)\n",
        "print(dt_confusion_matrix)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[713  27]\n",
            " [ 18   3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awlgUsEk7OmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute primary metrics for decision tree\n",
        "# True Positive\n",
        "dt_true_positive = dt_confusion_matrix[1,1]\n",
        "# True Negative\n",
        "dt_true_negative = dt_confusion_matrix[0,0]\n",
        "# False Positive\n",
        "dt_false_positive = dt_confusion_matrix[0,1]\n",
        "# False Negative\n",
        "dt_false_negative = dt_confusion_matrix[1,0]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvqChcMq7OmT",
        "colab_type": "text"
      },
      "source": [
        "### Confusion matrix\n",
        "A confusion matrix is visualization technique to summarize the basic performance of a classification algorithm.\n",
        "\n",
        "![Confusion matrix](https://static.packt-cdn.com/products/9781838555078/graphics/C13314_06_05.jpg \"Confusion matric diagram\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjUjqxpU7OmU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "outputId": "0226b259-9034-42ce-bd11-b85897cc95cb"
      },
      "source": [
        "# Plot confusion matrix, DO NOT EDIT THE CELL\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(25,8))\n",
        "\n",
        "plt.title(\"Confusion matrix for logistic regression\")\n",
        "sns.heatmap(np.array([[lr_true_negative, lr_false_positive],[lr_false_negative, lr_true_positive]]), annot=True, cmap=plt.cm.Blues, fmt='g', ax=axes[0])\n",
        "plt.title(\"Confusion matrix for decision tree\")\n",
        "sns.heatmap(np.array([[dt_true_negative, dt_false_positive],[dt_false_negative, dt_true_positive]]), annot=True, cmap=plt.cm.Blues, fmt='g', ax=axes[1])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x576 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njjI0xUK7OmZ",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy \n",
        "Classification accuracy is simply the rate of correct classifications\n",
        "$$Accuracy = \\frac{Number \\, of \\, correct \\, predictions}{Total \\, number \\, of \\, predictions}$$\n",
        "<br>\n",
        "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvvTWQHU7OmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classification accuracy for logistic regression\n",
        "lr_accuracy = (lr_true_positive+lr_true_negative)/(lr_true_positive + lr_true_negative + lr_false_positive + lr_false_negative)\n",
        "# Classification accuracy for decision tree\n",
        "dt_accuracy = (dt_true_positive + dt_true_negative) / (dt_true_positive + dt_true_negative + dt_false_positive + dt_false_negative)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swep8XGX7Omd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3d63ecbd-48a1-433c-bd8f-1a6c2accf62a"
      },
      "source": [
        "print(\"Classificaton accuracy: LR = \" , lr_accuracy)\n",
        "print(\"Classificaton accuracy: DT = \" , dt_accuracy)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classificaton accuracy: LR =  0.961892247043364\n",
            "Classificaton accuracy: DT =  0.9408672798948752\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCpSUVJm7Omg",
        "colab_type": "text"
      },
      "source": [
        "### Precision\n",
        "What proportion of positive identifications was actually correct?\n",
        "$$Precision = \\frac{TP}{TP+FP}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORBO9r8P7Omg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Precision for logistic regression\n",
        "try:\n",
        "    lr_precision = lr_true_positive / (lr_true_positive + lr_false_positive)\n",
        "except:\n",
        "    lr_precision = 0\n",
        "    print(\"If you see this message, it means that the\\ndenominator of precision for logistic regression turned out to be 0 \")\n",
        "# Precision for decision tree\n",
        "try:\n",
        "    dt_precision = dt_true_positive / (dt_true_positive + dt_false_positive)\n",
        "except:\n",
        "    dt_precision = 0\n",
        "    print(\"If you see this message, it means that the\\ndenominator of precision for decision tree turned out to be 0 \")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEbn8x9r7Oml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "150e2322-e795-4686-e954-adc20cd0604c"
      },
      "source": [
        "print(\"Precision: LR = \" , lr_precision)\n",
        "print(\"Precision: DT = \" , dt_precision)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: LR =  0.21428571428571427\n",
            "Precision: DT =  0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo6fdHVd7Omo",
        "colab_type": "text"
      },
      "source": [
        "### Recall\n",
        "What proportion of actual positives was identified correctly?\n",
        "$$Recall = \\frac{TP}{TP+FN}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt0xZV5g7Omp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recall for logistic regression\n",
        "try:\n",
        "    lr_recall = lr_true_positive / (lr_true_positive + lr_false_negative)\n",
        "except:\n",
        "    lr_recall = 0\n",
        "    print(\"If you see this message, it means that the\\ndenominator of recall for logistic regression turned out to be 0 \")\n",
        "# Recall for decision tree\n",
        "try:\n",
        "    dt_recall = dt_true_positive / (dt_true_positive + dt_false_negative)\n",
        "except:\n",
        "    dt_recall = 0\n",
        "    print(\"If you see this message, it means that the\\ndenominator of recall for decision tree turned out to be 0 \")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44b27lXl7Omt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ae62bf9a-d503-4aae-8c0e-e94401d00191"
      },
      "source": [
        "print(\"Recall: LR = \" , lr_recall)\n",
        "print(\"Recall: DT = \" , dt_recall)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recall: LR =  0.14285714285714285\n",
            "Recall: DT =  0.14285714285714285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCTD8YGO7Omz",
        "colab_type": "text"
      },
      "source": [
        "### F1 score\n",
        "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal.\n",
        "$$ F1 \\, score = \\frac{2* Precision * Recall}{Precision + Recall}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHTKTfIu7Omz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# F1 score for logistic regression\n",
        "lr_f1_score = (2 * lr_precision * lr_recall) / (lr_precision + lr_recall)\n",
        "# F1 score for decision tree\n",
        "dt_f1_score = (2 * dt_precision * dt_recall) / (dt_precision + dt_recall)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NEYKNFQ7Om2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0e189da9-133d-4fdf-c6f1-25b88f03ce78"
      },
      "source": [
        "print(\"F1 score: LR = \" , lr_f1_score)\n",
        "print(\"F1 score: DT = \" , dt_f1_score)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score: LR =  0.17142857142857143\n",
            "F1 score: DT =  0.11764705882352941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EidDR_Q-7Om6",
        "colab_type": "text"
      },
      "source": [
        "### Area Under ROC Curve\n",
        "A ROC Curve is a plot of the true positive rate and the false positive rate for a given set of probability predictions at different thresholds used to map the probabilities to class labels. The area under the curve is then the approximate integral under the ROC Curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5QvRPh47Om7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# False positive rate for logisitic regression\n",
        "lr_false_positive_rate = \n",
        "# True positive rate for logisitic regression\n",
        "lr_true_positive_rate = \n",
        "# False positive rate for decision trees\n",
        "dt_false_positive_rate = \n",
        "# True positive rate for decision trees\n",
        "dt_true_positive_rate = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZHZ-jVD7Om9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Plot the ROC curve\n",
        "plt.plot()\n",
        "plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mt_Azgd7OnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}